{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "20180524\n",
    "\n",
    "Adding a censor label for calculation of the C-index and for future train improvements\n",
    "\n",
    "Adding sample_type_id which must be 01A for the primary tumor\n",
    "\n",
    "20180525\n",
    "\n",
    "Fixed the Data Loader so that we sort on filename before calling.  Then our indices work and our loading of the file works also. \n",
    "\n",
    "20180530\n",
    "\n",
    "Using new sampler from tumor_classifier where images are a fixed size.  In tumor_classifier we can get 99% accuracy with tumor/normal classes\n",
    "\n",
    "Turn into classifier of > 1000 days (~ 3 years)\n",
    "\n",
    "\n",
    "try poisson loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastai\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import tqdm\n",
    "import os\n",
    "import json\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import resnet34, resnet152, transforms_top_down, CropType, \\\n",
    "    tfms_from_model, ConvLearner, optim, T\n",
    "from fastai.dataset import ImageClassifierData, Denormalize\n",
    "from fastai.metrics import accuracy, f1\n",
    "from fastai.sgdr import TrainingPhase, DecayType\n",
    "from lifelines.utils import concordance_index\n",
    "from torch.nn.modules.loss import PoissonNLLLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cindex_metric(preds, targs):\n",
    "    try:\n",
    "        cindex = concordance_index(np.exp(targs.cpu().numpy()).astype(int), \n",
    "                                   np.exp(preds.cpu().numpy()).astype(int))\n",
    "    except:\n",
    "        cindex = 0.0\n",
    "    return cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark=True\n",
    "torch.cuda.set_device(1)\n",
    "torch.cuda.current_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIVER_PATH = Path('/DATA/BIO/GDC/liver')\n",
    "LIVER_SAMPLES = LIVER_PATH/\"samples\"\n",
    "EXP_PATH = LIVER_PATH/\"exp_poisson\"\n",
    "EXP_MODEL_PATH = EXP_PATH/\"models\"\n",
    "EXP_DATA = EXP_PATH/\"data\"\n",
    "EXP_TRAIN_DATA = EXP_DATA/\"train\"\n",
    "EXP_TEST_DATA = EXP_DATA/\"test\"\n",
    "PATIENT_JSON = EXP_PATH/'patient_split.json'\n",
    "TRAIN_CSV = EXP_PATH/'level_1_train.csv'\n",
    "TRAIN_CSV_FULL = EXP_PATH/'level_1_train_full.csv'\n",
    "TEST_CSV = EXP_PATH/'level_1_test.csv'\n",
    "TEST_CSV_FULL = EXP_PATH/'level_1_test_FULL.csv'\n",
    "\n",
    "for d in [EXP_PATH, EXP_DATA, EXP_TRAIN_DATA, EXP_TEST_DATA, EXP_MODEL_PATH]:\n",
    "    if not d.exists():\n",
    "        d.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "slides = pd.read_csv(LIVER_PATH/'slides.csv')\n",
    "slides = slides.loc[slides.sample_type_id == 1].copy()\n",
    "slides['days_proxy'] = slides.days_to_death.fillna(slides.days_to_last_follow_up).astype(float)\n",
    "slides = slides.loc[slides.days_proxy.notnull()].copy()\n",
    "slides['event_observed'] = True\n",
    "slides.loc[slides.days_to_last_follow_up.notnull(),'event_observed'] = False    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263 79 113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7cd030391a435ea46d8031c0a57f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=399), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(PATIENT_JSON, 'r') as fobj:\n",
    "        patients = json.load(fobj)\n",
    "        train_patients = patients['train']\n",
    "        val_patients = patients['val']\n",
    "        test_patients = patients['test']\n",
    "    train_slides = slides.loc[slides.submitter_id.isin(train_patients)]\n",
    "    test_slides = slides.loc[slides.submitter_id.isin(test_patients)]\n",
    "\n",
    "    train_csv_df = pd.read_csv(TRAIN_CSV_FULL)\n",
    "    val_idx = train_csv_df.loc[train_csv_df.val_patient].index\n",
    "except:\n",
    "    patients = random.permutation(list(set(slides.submitter_id)))\n",
    "    \n",
    "    split     = int(0.7 * len(patients))\n",
    "    val_split = int(0.7 * split)\n",
    "    train_patients = patients[:split]\n",
    "    val_patients   = patients[val_split:split]\n",
    "    test_patients  = patients[split:]\n",
    "\n",
    "    patient_split = {\n",
    "        'train': list(train_patients),\n",
    "        'val': list(val_patients),\n",
    "        'test': list(test_patients)\n",
    "    }\n",
    "    with open(EXP_PATH/'patient_split.json', 'w') as fobj:\n",
    "        json.dump(patient_split, fobj)\n",
    "\n",
    "    print(len(train_patients), len(val_patients), len(test_patients))\n",
    "    \n",
    "    train_slides = slides.loc[slides.submitter_id.isin(train_patients)]\n",
    "    test_slides  = slides.loc[slides.submitter_id.isin(test_patients)]\n",
    "    \n",
    "    \n",
    "    train_items = []\n",
    "    test_items = []\n",
    "    num_slides = []\n",
    "    slide_level = 'level_1'\n",
    "    samples_per_slide = 20\n",
    "    \n",
    "    \n",
    "    for ix, patient in tqdm.tqdm_notebook(slides.iterrows(), total=len(slides)):\n",
    "        sfp = LIVER_SAMPLES/patient.slide_file_name.upper()/slide_level\n",
    "        sample_files = list(sfp.iterdir())\n",
    "        num_samples = len(sample_files)\n",
    "        num_slides.append(num_samples)\n",
    "        for fn in np.random.choice(sample_files, size=min(samples_per_slide,num_samples), replace=False):\n",
    "            if patient.submitter_id in train_patients:\n",
    "                dest_path = EXP_TRAIN_DATA\n",
    "                train = True\n",
    "                if patient.submitter_id in val_patients:\n",
    "                    val = True\n",
    "                else:\n",
    "                    val = False\n",
    "            else:\n",
    "                dest_path = EXP_TEST_DATA\n",
    "                train = False\n",
    "\n",
    "            slide_id = os.path.basename(patient.slide_file_name).split('.')[0]\n",
    "            img_id = os.path.basename(fn).split('.')[0]\n",
    "            new_fn_base = '-'.join([slide_id, img_id]) + '.tiff'\n",
    "            full_path = dest_path/new_fn_base\n",
    "\n",
    "            if not full_path.exists():\n",
    "                os.symlink(fn, dest_path/new_fn_base)\n",
    "\n",
    "            if train:\n",
    "                train_items.append({\n",
    "                    'fn': str(new_fn_base),\n",
    "                    'val': np.log(patient.days_proxy+1),\n",
    "                    'val_patient': val,\n",
    "                    'patient_id': patient.submitter_id,\n",
    "                    'slide_file':patient.slide_file_name\n",
    "                })\n",
    "            else:\n",
    "                test_items.append({\n",
    "                    'fn': str(new_fn_base),\n",
    "                    'val': np.log(patient.days_proxy+1),\n",
    "                    'patient_id': patient.submitter_id,\n",
    "                    'slide_file':patient.slide_file_name\n",
    "                })\n",
    "       \n",
    "    train_csv_df = pd.DataFrame(list(random.permutation(train_items)))\n",
    "     #fast.ai will sort on filenames, idx will be a mess!\n",
    "    train_csv_df = train_csv_df.sort_values('fn').reset_index(drop=True)\n",
    "    \n",
    "    train_csv_df[['fn','val']].to_csv(TRAIN_CSV, index=False)\n",
    "    train_csv_df.to_csv(TRAIN_CSV_FULL, index=False)\n",
    "    val_idx = train_csv_df.loc[train_csv_df.val_patient].index\n",
    "    \n",
    "    test_csv_df = pd.DataFrame(test_items)\n",
    "    test_csv_df = test_csv_df.sort_values('fn').reset_index(drop=True)\n",
    "    \n",
    "    test_csv_df[['fn','val']].to_csv(TEST_CSV, index=False)\n",
    "    test_csv_df.to_csv(TEST_CSV_FULL, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model = resnet152\n",
    "sz=256\n",
    "bs=32\n",
    "tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_top_down, crop_type=CropType.CENTER)\n",
    "md = ImageClassifierData.from_csv(\n",
    "    EXP_PATH, \"data/train\", TRAIN_CSV, tfms=tfms, bs=bs, val_idxs=val_idx.values, continuous=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    test_csv_df\n",
    "except NameError:\n",
    "    test_csv_df = pd.read_csv(TEST_CSV_FULL)\n",
    "    \n",
    "md_test = ImageClassifierData.from_csv(\n",
    "    EXP_PATH, \"data/test\", TEST_CSV, tfms=tfms, bs=bs, val_idxs=test_csv_df.index.values, continuous=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.pretrained(f_model, md)\n",
    "learn.opt_fn = optim.Adam\n",
    "#learn.crit = PoissonNLLLoss(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Dropout(p=0.5),\n",
       " Linear(in_features=512, out_features=1, bias=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.children[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to true to try learning rate finder\n",
    "if False:\n",
    "    learn.save('tmp')\n",
    "    learn.unfreeze()\n",
    "    lrf=learn.lr_find()\n",
    "    learn.sched.plot(0)\n",
    "    learn.load('tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    }
   ],
   "source": [
    "lr = 10e-3\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1206eb95d4442e8b8638a2eaba52dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   cindex_metric              \n",
      "    0      4.681112   1.89927    0.494228  \n",
      "    1      1.600315   0.992847   0.457846                   \n",
      "    2      1.213423   4.976754   0.476419                   \n",
      "    3      1.157554   6.679834   0.503424                   \n",
      "    4      1.118454   1.282096   0.507104                   \n",
      " 40%|████      | 50/125 [00:37<00:55,  1.35it/s, loss=1.14]"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit(lr, 1, cycle_len=100, use_clr=(100,10), best_save_name='liver_class_best_1', metrics=[cindex_metric])\n",
    "learn.save('liver_class_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('liver_class_best_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.load('liver_class_best_1')\n",
    "learn.save('liver_saveme')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, targ = learn.predict_with_targs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance_index(np.exp(targ), np.exp(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(np.exp(y_pred), targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.exp(y_pred[:,0])).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(poisson.pmf(np.arange(max_count), _lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.loc[:,'y_pred']=y_pred_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.clip(val_df.y_pred,0,np.inf),val_df.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(concordance_index(val_df.y_pred,val_df.val))\n",
    "print(cindex_metric(val_df.y_pred,val_df.val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = learn.predict_dl(md_test.val_dl)\n",
    "test_df = test_csv_df.copy()\n",
    "test_df['y_pred'] = y_pred\n",
    "\n",
    "y_pred.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp_pred = test_df.groupby('patient_id').y_pred.min()\n",
    "yp_targ = test_df.groupby('patient_id').val.mean()\n",
    "concordance_index(yp_pred, yp_targ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(yp_targ, yp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.val.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_df.val.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.poisson_nll_loss??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance_index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "499px",
    "left": "799.5px",
    "right": "115px",
    "top": "128px",
    "width": "653px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
