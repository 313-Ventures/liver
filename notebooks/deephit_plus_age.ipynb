{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastai\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import tqdm\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "from fastai.conv_learner import resnet34,resnext101, transforms_top_down, CropType, \\\n",
    "    tfms_from_model, ConvLearner, optim, T, Callback, RandomRotateZoom, to_gpu, \\\n",
    "    Learner, BasicModel\n",
    "from fastai.dataset import Denormalize, ImageData, FilesNhotArrayDataset, \\\n",
    "    ImageClassifierData, csv_source, parse_csv_labels, split_by_idx, read_dir, \\\n",
    "    FilesIndexArrayDataset, dict_source, FilesArrayDataset, DataLoader, ModelData, ImageData\n",
    "from fastai.layers import AdaptiveConcatPool2d,Flatten\n",
    "from torch.nn import BatchNorm1d,Dropout,ReLU,Linear,Sequential,Hardtanh,Softmax\n",
    "from fastai.column_data import PassthruDataset\n",
    "from fastai.sgdr import TrainingPhase, DecayType\n",
    "from lifelines.utils import concordance_index\n",
    "from collections import defaultdict\n",
    "from aixtras import *\n",
    "from bio.liver import build_liver_csv_data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "devs = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_liver_csv_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIVER_PATH = Path('/DATA/BIO/GDC/liver')\n",
    "LIVER_SAMPLES = LIVER_PATH/\"samples\"\n",
    "EXP_PATH = LIVER_PATH/\"exp_deep_play\"\n",
    "EXP_MODEL_PATH = EXP_PATH/\"models\"\n",
    "TRAIN_DIR = EXP_PATH/\"train\"\n",
    "TEST_DIR = EXP_PATH/\"test\"\n",
    "CSV_DATA = EXP_PATH/\"records.csv\"\n",
    "TB_LOGS = EXP_PATH/'logs'\n",
    "\n",
    "force_rebuild = True\n",
    "scale_x_cols = False\n",
    "    \n",
    "for d in [EXP_PATH, EXP_MODEL_PATH, TB_LOGS]:\n",
    "    if not d.exists():\n",
    "        d.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "sw = SummaryWriter('/DATA/tblogs/runs/expplay')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = build_liver_csv_data(\n",
    "    LIVER_PATH, \n",
    "    EXP_PATH, \n",
    "    TRAIN_DIR, \n",
    "    TEST_DIR, \n",
    "    progress=tqdm.tqdm_notebook,\n",
    "    force_rebuild=force_rebuild,\n",
    "    samples_per_patient=50,\n",
    "    val_split=0.5\n",
    ")\n",
    "\n",
    "# remember largest possible survival day\n",
    "t_max = int(csv_data.event_time.max()) # np.int64 will fuck up torch\n",
    "print(t_max)\n",
    "\n",
    "dframes = {}\n",
    "for dsname, df in csv_data.groupby('dsname'):\n",
    "    dframes[dsname] = df\n",
    "\n",
    "\n",
    "feat_cols = ['age_at_diagnosis']\n",
    "n_cols = len(feat_cols)\n",
    "\n",
    "if scale_x_cols:\n",
    "    scaler = MinMaxScaler().fit(dframes['train'][feat_cols].values.astype(float))\n",
    "    for k in dframes:\n",
    "        dframes[k].loc[:,feat_cols] = scaler.transform(dframes[k].loc[:,feat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dframes['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilesAndColumnsSurvivalDataset(FilesArrayDataset):\n",
    "    def __init__(self, fnames, x_cols, y_times_and_observed, t_max, transform, path, jitter=None):\n",
    "        self.y=y_times_and_observed\n",
    "        self.t_max = t_max\n",
    "        self.x_cols = x_cols\n",
    "        self.n_cols = x_cols.shape[0]\n",
    "        self.fnames = fnames\n",
    "        self.jitter = jitter\n",
    "        super().__init__(fnames, self.y, transform, path)\n",
    "        assert(len(self.fnames)==len(self.y)==len(self.x_cols))\n",
    "        self.c = self.get_c()\n",
    "        \n",
    "    def get_y(self, i): return self.y[i]\n",
    "    \n",
    "    def get_c(self):\n",
    "        return self.t_max\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        x_img = super().get_x( i)\n",
    "        x_cols = self.x_cols[i]\n",
    "        return [x_cols, x_img]\n",
    "    \n",
    "    def get1item(self, idx):\n",
    "        x,y = self.get_x(idx),self.get_y(idx)\n",
    "        x_cols, x_img = x\n",
    "        if self.jitter:\n",
    "            jitter = np.random.randn(*x_cols.shape) * self.jitter + 1.0\n",
    "            x_cols *= jitter\n",
    "        else:\n",
    "            x_cols = x_cols.astype(float)\n",
    "        x_cols = np.log(x_cols + 0.01) - 6.0\n",
    "        x_img, y = self.get(self.transform, x_img, y)\n",
    "        x_flat = np.concatenate([x_cols.flatten(), x_img.flatten()])\n",
    "        #return x_flat, y\n",
    "        return x_cols, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz, bs, tfms, num_workers=8):\n",
    "    def df_to_ds(key, tfm, jitter=None):\n",
    "        df = dframes[key]\n",
    "        ds = FilesAndColumnsSurvivalDataset(\n",
    "                df.dest_tile.values, \n",
    "                df[feat_cols].values, \n",
    "                df[['event_time','event_type']].values, \n",
    "                t_max, \n",
    "                tfm,\n",
    "                EXP_PATH,\n",
    "                jitter=jitter\n",
    "        )\n",
    "        return ds\n",
    "\n",
    "    \n",
    "    datasets = [\n",
    "        df_to_ds('train', tfms[0]), #, jitter=0.03),\n",
    "        df_to_ds('valid', tfms[1]),\n",
    "        df_to_ds('train', tfms[1]),\n",
    "        df_to_ds('valid', tfms[0]), #, jitter=0.03),\n",
    "        df_to_ds('test', tfms[1]),\n",
    "        df_to_ds('test', tfms[0]) #, jitter=0.03)\n",
    "    ]\n",
    "    \n",
    "    classes = range(t_max+1)\n",
    "    model_data = ImageData(EXP_PATH, datasets, bs, num_workers, classes)\n",
    "    return model_data\n",
    "\n",
    "\n",
    "num_evt_types = 1\n",
    "def custom_loss(preds, target):\n",
    "    step += 1\n",
    "    #import pdb; pdb.set_trace()\n",
    "    evt_times = target[:,0]\n",
    "    evt_types = target[:,1]\n",
    "    l1_loss, pairwise_loss = deephit_loss(preds, evt_times, evt_types, t_max+1, num_evt_types)\n",
    "    b1 = 0.50\n",
    "    b2 = 0.50\n",
    "    return b1 * pairwise_loss + b2 * l1_loss \n",
    "\n",
    "\n",
    "class ConcordanceIndex(Callback):\n",
    "    def __init__(self, ):\n",
    "        self.reset()\n",
    "\n",
    "    def on_epoch_begin(self, metrics):\n",
    "        self.reset()\n",
    "\n",
    "    def on_epoch_end(self, metrics):\n",
    "        ci = concordance_index(\n",
    "            np.array(self.evt_times), \n",
    "            np.array(self.preds), \n",
    "            np.array(self.evt_types)\n",
    "        )\n",
    "        #import pdb; pdb.set_trace()\n",
    "        print('ci: ', ci, len(self.preds), len(self.evt_times))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.preds = []\n",
    "        self.evt_times = []\n",
    "        self.evt_types = []\n",
    "        self.mcount = 0\n",
    "       \n",
    "    def concordance_metric(self, preds, target):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        self.evt_times += list(target[:,0])\n",
    "        self.evt_types += list(target[:,1])\n",
    "        self.preds += list(np.argmax(preds, axis=1))\n",
    "        self.mcount += 1\n",
    "        return 0.0 #self.mcount    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model = resnet34\n",
    "bs = 32\n",
    "sz = 256\n",
    "tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_top_down) \n",
    "md = get_data(sz, bs, tfms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#md.trn_ds[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitCols(nn.Module):\n",
    "    def __init__(self, img_model, cols_model, sz, n_chan, n_cols):\n",
    "        super().__init__()\n",
    "        self.sz = sz\n",
    "        self.n_cols = n_cols\n",
    "        self.n_chan = n_chan\n",
    "        self.img_model = img_model\n",
    "        self.cols_model = cols_model\n",
    "        self.final = nn.Sequential(\n",
    "            #nn.Linear(in_features=2*len(md.classes), out_features=len(md.classes)),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        bs = x.shape[0]\n",
    "        x_cols = x[:,0:self.n_cols].astype(float)\n",
    "        #x_img = x[:,self.n_cols:].reshape((bs,self.n_chan,sz,sz))\n",
    "        #img_result = self.img_model(x_img)\n",
    "        cols_result = cols_model(x_cols)\n",
    "        #x_combo = torch.cat((cols_result, img_result), dim=1)\n",
    "        #x_combo = cols_result + img_result\n",
    "        x_combo = cols_result #+ img_result\n",
    "        return cols_result\n",
    "        #return self.final(x_combo)\n",
    "\n",
    "\n",
    "feat = 1024\n",
    "\n",
    "layers = [AdaptiveConcatPool2d(), Flatten()]\n",
    "layers += [\n",
    "    BatchNorm1d(feat),\n",
    "    Dropout(p=0.5), \n",
    "    Linear(in_features=feat, out_features=256), \n",
    "    ReLU(), \n",
    "    BatchNorm1d(256),\n",
    "    Dropout(p=0.5), \n",
    "    Linear(in_features=256, out_features=len(md.classes))\n",
    "]\n",
    "custom_head = Sequential(*layers)\n",
    "\n",
    "hidden = 256\n",
    "cols_model = nn.Sequential(\n",
    "    Linear(in_features=n_cols, out_features=hidden), \n",
    "    ReLU(), \n",
    "    BatchNorm1d(hidden),\n",
    "    Linear(in_features=hidden, out_features=len(md.classes)),\n",
    "    Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BasicModel(to_gpu(cols_model), 'colnet')\n",
    "\n",
    "learn = Learner(md, m)\n",
    "cindex = ConcordanceIndex()\n",
    "callbacks = [cindex]\n",
    "learn.crit = custom_loss\n",
    "learn.metrics = [cindex.concordance_metric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = learn.data.trn_ds[0:32]\n",
    "learn.models.model(T(x.astype(float))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cindex.reset()\n",
    "learn.fit(0.01, \n",
    "          1, cycle_len=5, \n",
    "          use_clr_beta = (40,20,0.95,0.85), \n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(preds).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConvLearner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5e2e55633b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvLearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcordanceIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcordance_metric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ConvLearner' is not defined"
     ]
    }
   ],
   "source": [
    "learn = ConvLearner.pretrained(f_model, md, custom_head=custom_head)\n",
    "cindex = ConcordanceIndex()\n",
    "callbacks = [cindex]\n",
    "learn.crit = custom_loss\n",
    "learn.metrics = [cindex.concordance_metric]\n",
    "\n",
    "learn.unfreeze()\n",
    "learn.models.model = to_gpu(SplitCols(learn.models.model, cols_model, sz, 3, n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = learn.data.trn_ds[0:32]\n",
    "learn.models.model(T(x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = learn.data.trn_ds[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.models.model(T(x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cindex.reset()\n",
    "learn.fit(0.1, \n",
    "          1, cycle_len=10, \n",
    "          use_clr_beta = (40,20,0.95,0.85), \n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
