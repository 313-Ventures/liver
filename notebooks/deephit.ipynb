{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastai\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import tqdm\n",
    "import os\n",
    "import json\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "\n",
    "from fastai.conv_learner import resnet34,resnext101, transforms_top_down, CropType, \\\n",
    "    tfms_from_model, ConvLearner, optim, T, Callback\n",
    "from fastai.dataset import Denormalize, ImageData, FilesNhotArrayDataset, \\\n",
    "    ImageClassifierData, csv_source, parse_csv_labels, split_by_idx, read_dir, \\\n",
    "    FilesIndexArrayDataset, dict_source, FilesArrayDataset\n",
    "from fastai.sgdr import TrainingPhase, DecayType\n",
    "from lifelines.utils import concordance_index\n",
    "from collections import defaultdict\n",
    "from aixtras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True\n",
    "torch.cuda.set_device(1)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIVER_PATH = Path('/DATA/BIO/GDC/liver')\n",
    "LIVER_SAMPLES = LIVER_PATH/\"samples\"\n",
    "EXP_PATH = LIVER_PATH/\"exp_deep\"\n",
    "EXP_MODEL_PATH = EXP_PATH/\"models\"\n",
    "TRAIN_DIR = EXP_PATH/\"train\"\n",
    "TEST_DIR = EXP_PATH/\"test\"\n",
    "CSV_DATA = EXP_PATH/\"records.csv\"\n",
    "\n",
    "for d in [EXP_PATH, EXP_MODEL_PATH]:\n",
    "    if not d.exists():\n",
    "        d.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CSV_DATA.exists():\n",
    "    print(\"build traing/val/test csv data\")\n",
    "    \n",
    "    slides = pd.read_csv(LIVER_PATH/'slides.csv')\n",
    "    slides = slides.loc[slides.sample_type_id.isin([1,11])]\n",
    "    slide_level = 'level_1'\n",
    "    samples_per_patient = 100\n",
    "    split = 0.8\n",
    "    val_split = 0.8\n",
    "\n",
    "    slide_info = defaultdict(dict)\n",
    "\n",
    "    def pull_tiles(slides, patient_id, num_tiles, slide_level):\n",
    "        slide_fns = []\n",
    "        tiles = []\n",
    "\n",
    "        # get list of candidate samples\n",
    "        for i, row in slides.loc[slides.submitter_id == patient_id].iterrows():\n",
    "            slide_name = row.slide_file_name\n",
    "            sfp = LIVER_SAMPLES/row.slide_file_name.upper()/slide_level\n",
    "            slide_fns = list(sfp.iterdir())\n",
    "\n",
    "        num_samples = len(slide_fns)\n",
    "        tiles = list(np.random.choice(slide_fns, size=min(num_tiles,num_samples), replace=False))\n",
    "\n",
    "        return tiles\n",
    "\n",
    "\n",
    "\n",
    "    def build_tiles(patients, dsname, folder):\n",
    "        records = []\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "        for p in tqdm.tqdm_notebook(patients):\n",
    "            tiles = pull_tiles(slides, p, samples_per_patient, slide_level)\n",
    "            for i, tile_fn in enumerate(tiles):\n",
    "                base_name = '%s_%04d.tiff' % (p, i)\n",
    "                dest_tile = folder/base_name\n",
    "                os.symlink(tile_fn, dest_tile)\n",
    "                records.append({\n",
    "                    'patient': p,\n",
    "                    'dsname': dsname,\n",
    "                    'event_time': slides.loc[slides.submitter_id == p, 'days_proxy'].iloc[0],\n",
    "                    'event_type': slides.loc[slides.submitter_id == p, 'event_observed'].iloc[0],\n",
    "                    'src_tile': tile_fn,\n",
    "                    'dest_tile': dest_tile\n",
    "                })\n",
    "        return records\n",
    "\n",
    "\n",
    "\n",
    "    # create event time, drop any nulls, create event observed\n",
    "    slides['days_proxy'] = slides.days_to_death.fillna(slides.days_to_last_follow_up)\n",
    "    slides = slides.loc[slides.days_proxy.notnull()]\n",
    "    slides['event_observed'] = True\n",
    "    slides.loc[slides.days_to_last_follow_up.notnull(),'event_observed'] = False    \n",
    "    slides['event_observed'] = slides['event_observed'].astype(int)\n",
    "\n",
    "    # filter tumor only\n",
    "    slides = slides.loc[slides.sample_type_id == 1]\n",
    "\n",
    "    #create censor label\n",
    "\n",
    "    patients = list(set(slides.submitter_id))\n",
    "    num_patients = len(patients)\n",
    "    train_val_split = int(split * num_patients)\n",
    "    train_split = int(val_split * train_val_split)\n",
    "\n",
    "    random_patients = np.random.permutation(patients)\n",
    "    train_patients = random_patients[0:train_split]\n",
    "    valid_patients = random_patients[train_split:train_val_split]\n",
    "    test_patients = random_patients[train_val_split:]\n",
    "\n",
    "    # convert days_proxy to int for softmax\n",
    "    slides['days_proxy'] = slides.days_proxy.astype(int)\n",
    "\n",
    "\n",
    "    # arrange the sample data\n",
    "    train_records = build_tiles(train_patients, 'train', TRAIN_DIR)\n",
    "    valid_records = build_tiles(valid_patients, 'valid', TRAIN_DIR)\n",
    "    test_records = build_tiles(test_patients, 'test', TEST_DIR)\n",
    "\n",
    "    csv_data = pd.DataFrame(train_records + valid_records + test_records)\n",
    "    csv_data.to_csv(CSV_DATA, index=False)\n",
    "else:\n",
    "    print(\"csv data already built\")\n",
    "\n",
    "csv_data = pd.read_csv(CSV_DATA)\n",
    "csv_data.event_time = csv_data.event_time // 10\n",
    "# remember largest possible survival day\n",
    "t_max = int(csv_data.event_time.max()) # np.int64 will fuck up torch\n",
    "print(t_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSurvivalData(ImageClassifierData):\n",
    "    @classmethod\n",
    "    def from_suvival_csv(cls, path, folder, csv_fname, bs=64, tfms=(None,None),\n",
    "                         test_name=None, skip_header=True, num_workers=8, \n",
    "                         fname_col='fname', time_col='event_time', type_col='event_type', suffix=None):\n",
    "        assert not (tfms[0] is None or tfms[1] is None), \"please provide transformations for your train and validation sets\"\n",
    "        assert not (os.path.isabs(folder)), \"folder needs to be a relative path\"\n",
    "        \n",
    "        csv_data = pd.read_csv(csv_fname)\n",
    "        csv_data[time_col] = csv_data[time_col]\n",
    "        t_max = csv_data[time_col].max()\n",
    "        classes = list(range(t_max+1))\n",
    "        num_classes = len(classes)\n",
    " \n",
    "        train_val_data = csv_data.loc[csv_data.dsname.isin(['train','valid'])]\n",
    "        test_data = csv_data.loc[csv_data.dsname == 'test']\n",
    "        \n",
    "        fnames = train_val_data[fname_col]\n",
    "        test_fnames = test_data[fname_col]\n",
    "        \n",
    "        def get_one_hot(targets, nb_classes):\n",
    "            return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "\n",
    "        evt_times = train_val_data[time_col].values\n",
    "        evt_type = train_val_data[type_col].values\n",
    "        \n",
    "        #y = np.concatenate([evt_times, evt_type[:,None]], axis=1)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        y = train_val_data[[time_col, type_col]].values\n",
    "        y_test = test_data[[time_col, type_col]].values\n",
    "        \n",
    "        val_idxs = train_val_data.dsname == 'valid'\n",
    "        \n",
    "        ((val_fnames,trn_fnames),(val_y,trn_y)) = split_by_idx(val_idxs, np.array(fnames), y)\n",
    "         \n",
    "        class FilesSurvivalArrayDataset(FilesArrayDataset):\n",
    "            def get_c(self): return int(t_max + 1)\n",
    "\n",
    "            @property\n",
    "            def is_multi(self): return True\n",
    "    \n",
    "        f = FilesSurvivalArrayDataset\n",
    "        datasets = cls.get_ds(f, (trn_fnames,trn_y), (val_fnames,val_y), tfms,\n",
    "                               path=path, test=np.array(test_fnames))\n",
    "        \n",
    "        datasets[4].y = y_test\n",
    "        datasets[5].y = y_test\n",
    "        \n",
    "        return cls(path, datasets, bs, num_workers, classes=classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model = resnext101\n",
    "\n",
    "def get_data(sz, bs):\n",
    "    tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_top_down)\n",
    "    return ImageSurvivalData.from_suvival_csv(\n",
    "        EXP_PATH, 'train', CSV_DATA, test_name='test', \n",
    "        tfms=tfms, bs=bs, fname_col='dest_tile'\n",
    "    )\n",
    "\n",
    "sz=256\n",
    "bs=8\n",
    "md = get_data(sz, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.layers import AdaptiveConcatPool2d,Flatten\n",
    "from torch.nn import BatchNorm1d,Dropout,ReLU,Linear,Sequential,Hardtanh,Softmax\n",
    "\n",
    "feat = 4096\n",
    "\n",
    "layers = [AdaptiveConcatPool2d(), Flatten()]\n",
    "layers += [BatchNorm1d(feat),\n",
    "            Dropout(p=0.5), \n",
    "            Linear(in_features=feat, out_features=256), \n",
    "            ReLU(), \n",
    "            BatchNorm1d(256),\n",
    "            Dropout(p=0.5), \n",
    "            Linear(in_features=256, out_features=len(md.classes)),\n",
    "            Softmax()]\n",
    "head_relu = Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.pretrained(f_model, md,custom_head=head_relu)\n",
    "#learn = ConvLearner.pretrained(f_model, md)\n",
    "\n",
    "num_evt_types = 1\n",
    "def custom_loss(preds, target):\n",
    "    evt_times = target[:,0]\n",
    "    evt_types = target[:,1]\n",
    "    l1_loss, pairwise_loss = deephit_loss(preds, evt_times, evt_types, t_max+1, num_evt_types)\n",
    "    b1 = 0.50\n",
    "    b2 = 0.50\n",
    "    return b1 * pairwise_loss + b2 * l1_loss \n",
    "\n",
    "\n",
    "class ConcordanceIndex(Callback):\n",
    "    def __init__(self, ):\n",
    "        self.reset()\n",
    "\n",
    "    def on_epoch_begin(self, metrics):\n",
    "        self.reset()\n",
    "\n",
    "    def on_epoch_end(self, metrics):\n",
    "        ci = concordance_index(\n",
    "            np.array(self.evt_times), \n",
    "            np.array(self.preds), \n",
    "            np.array(self.evt_types)\n",
    "        )\n",
    "        print('ci: ', ci, len(self.preds), len(self.evt_times))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.preds = []\n",
    "        self.evt_times = []\n",
    "        self.evt_types = []\n",
    "        self.mcount = 0\n",
    "       \n",
    "    def concordance_metric(self, preds, target):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        self.evt_times += list(target[:,0])\n",
    "        self.evt_types += list(target[:,1])\n",
    "        self.preds += list(np.argmax(preds, axis=1))\n",
    "        self.mcount += 1\n",
    "        return 0.0 #self.mcount    \n",
    "\n",
    "\n",
    "cindex = ConcordanceIndex()\n",
    "callbacks = [cindex]\n",
    "learn.crit = custom_loss\n",
    "learn.metrics = [cindex.concordance_metric] # accuracy stuff gets confused by last column of evt_type\n",
    "#learn.opt_fn = optim.Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.children[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    learn.save('tmp')\n",
    "    lrf=learn.lr_find()\n",
    "    learn.sched.plot(0)\n",
    "    learn.load('tmp')\n",
    "    cindex.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "learn.fit(lr, 5, cycle_len=1, \n",
    "          use_clr_beta = (40,20,0.95,0.85), \n",
    "          best_save_name='liver_deephit_1_best',\n",
    "          callbacks = callbacks) \n",
    "\n",
    "learn.save('liver_deephit_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('liver_deephit_1')\n",
    "cindex.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "lr = 0.00003\n",
    "learn.fit(lr, 1, cycle_len=10, \n",
    "          use_clr_beta = (40,20,0.95,0.85), \n",
    "          best_save_name='liver_deephit_2_best',\n",
    "          callbacks = callbacks) \n",
    "learn.save('liver_deephit_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('liver_deephit_2')\n",
    "cindex.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_preds, targs = learn.TTA()\n",
    "preds = np.mean(multi_preds, 0)\n",
    "y_pred = np.argmax(preds, 1)\n",
    "evt_type = targs[:, 1]\n",
    "evt_time = targs[:, 0]\n",
    "concordance_index(evt_time, y_pred, evt_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_preds, targs = learn.TTA(is_test=True)\n",
    "preds = np.mean(multi_preds, 0)\n",
    "y_pred = np.argmax(preds, 1)\n",
    "evt_type = targs[:, 1]\n",
    "evt_time = targs[:, 0]\n",
    "concordance_index(evt_time, y_pred, evt_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "lr = 0.0001\n",
    "learn.fit(lr, 1, cycle_len=300, \n",
    "          use_clr_beta = (40,20,0.95,0.85), \n",
    "          best_save_name='liver_deephit_3_best',\n",
    "          callbacks = callbacks) \n",
    "learn.save('liver_deephit_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('liver_deephit_3')\n",
    "\n",
    "multi_preds, targs = learn.TTA()\n",
    "preds = np.mean(multi_preds, 0)\n",
    "y_pred = np.argmax(preds, 1)\n",
    "evt_type = targs[:, 1]\n",
    "evt_time = targs[:, 0]\n",
    "print(concordance_index(evt_time, y_pred, evt_type))\n",
    "\n",
    "multi_preds, targs = learn.TTA(is_test=True)\n",
    "preds = np.mean(multi_preds, 0)\n",
    "y_pred = np.argmax(preds, 1)\n",
    "evt_type = targs[:, 1]\n",
    "evt_time = targs[:, 0]\n",
    "print(concordance_index(evt_time, y_pred, evt_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
