{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastai\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import tqdm\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "from fastai.conv_learner import resnet34,resnext101, transforms_top_down, CropType, \\\n",
    "    tfms_from_model, ConvLearner, optim, T, Callback, RandomRotateZoom, to_gpu,to_np\n",
    "from fastai.dataset import Denormalize, ImageData, FilesNhotArrayDataset, \\\n",
    "    ImageClassifierData, csv_source, parse_csv_labels, split_by_idx, read_dir, \\\n",
    "    FilesIndexArrayDataset, dict_source, FilesArrayDataset, DataLoader, ModelData, ImageData\n",
    "from fastai.layers import AdaptiveConcatPool2d,Flatten\n",
    "from torch.nn import BatchNorm1d,Dropout,ReLU,Linear,Sequential,Hardtanh,Softmax\n",
    "from fastai.column_data import PassthruDataset\n",
    "from fastai.sgdr import TrainingPhase, DecayType\n",
    "from lifelines.utils import concordance_index\n",
    "from collections import defaultdict\n",
    "from aixtras import *\n",
    "from bio.liver import build_liver_csv_data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(7)\n",
    "\n",
    "devs = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIVER_PATH = Path('/DATA/BIO/GDC/liver')\n",
    "LIVER_SAMPLES = LIVER_PATH/\"samples\"\n",
    "EXP_PATH = LIVER_PATH/\"exp_deep_play\"\n",
    "EXP_MODEL_PATH = EXP_PATH/\"models\"\n",
    "TRAIN_DIR = EXP_PATH/\"train\"\n",
    "TEST_DIR = EXP_PATH/\"test\"\n",
    "CSV_DATA = EXP_PATH/\"records.csv\"\n",
    "\n",
    "force_rebuild = False\n",
    "\n",
    "    \n",
    "for d in [EXP_PATH, EXP_MODEL_PATH]:\n",
    "    if not d.exists():\n",
    "        d.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv data already built\n",
      "3675\n"
     ]
    }
   ],
   "source": [
    "csv_data = build_liver_csv_data(\n",
    "    LIVER_PATH, \n",
    "    EXP_PATH, \n",
    "    TRAIN_DIR, \n",
    "    TEST_DIR, \n",
    "    progress=tqdm.tqdm_notebook,\n",
    "    force_rebuild=force_rebuild,\n",
    "    samples_per_patient=50\n",
    ")\n",
    "\n",
    "# remember largest possible survival day\n",
    "t_max = int(csv_data.event_time.max()) # np.int64 will fuck up torch\n",
    "print(t_max)\n",
    "\n",
    "dframes = {}\n",
    "for dsname, df in csv_data.groupby('dsname'):\n",
    "    dframes[dsname] = df\n",
    "\n",
    "\n",
    "feat_cols = ['age_at_diagnosis']\n",
    "n_cols = len(feat_cols)\n",
    "scaler = MinMaxScaler().fit(dframes['train'][feat_cols].values.astype(float))\n",
    "for k in dframes:\n",
    "    dframes[k].loc[:,feat_cols] = scaler.transform(dframes[k].loc[:,feat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilesAndColumnsSurvivalDataset(FilesArrayDataset):\n",
    "    def __init__(self, fnames, x_cols, y_times_and_observed, t_max, transform, path, jitter=None):\n",
    "        self.y=y_times_and_observed\n",
    "        self.t_max = t_max\n",
    "        self.x_cols = x_cols\n",
    "        self.n_cols = x_cols.shape[0]\n",
    "        self.fnames = fnames\n",
    "        self.jitter = jitter\n",
    "        super().__init__(fnames, self.y, transform, path)\n",
    "        assert(len(self.fnames)==len(self.y)==len(self.x_cols))\n",
    "        self.c = self.get_c()\n",
    "        \n",
    "    def get_y(self, i): return self.y[i]\n",
    "    \n",
    "    def get_c(self):\n",
    "        return self.t_max\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        x_img = super().get_x( i)\n",
    "        x_cols = self.x_cols[i]\n",
    "        return [x_cols, x_img]\n",
    "    \n",
    "    def get1item(self, idx):\n",
    "        x,y = self.get_x(idx),self.get_y(idx)\n",
    "        x_cols, x_img = x\n",
    "        if self.jitter:\n",
    "            jitter = np.random.randn(*x_cols.shape) * self.jitter + 1.0\n",
    "            x_cols *= jitter\n",
    "\n",
    "        x_img, y = self.get(self.transform, x_img, y)\n",
    "        x_flat = np.concatenate([x_cols.flatten(), x_img.flatten()])\n",
    "        return x_flat, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz, bs, tfms, num_workers=8):\n",
    "    def df_to_ds(key, tfm, jitter=None):\n",
    "        df = dframes[key]\n",
    "        ds = FilesAndColumnsSurvivalDataset(\n",
    "                df.dest_tile.values, \n",
    "                df[feat_cols].values, \n",
    "                df[['event_time','event_type']].values, \n",
    "                t_max, \n",
    "                tfm,\n",
    "                EXP_PATH,\n",
    "                jitter=jitter\n",
    "        )\n",
    "        return ds\n",
    "\n",
    "    \n",
    "    datasets = [\n",
    "        df_to_ds('train', tfms[0], jitter=None), #0.01),\n",
    "        df_to_ds('valid', tfms[1]),\n",
    "        df_to_ds('train', tfms[1]),\n",
    "        df_to_ds('valid', tfms[0], jitter=None), #0.01),\n",
    "        df_to_ds('test', tfms[1]),\n",
    "        df_to_ds('test', tfms[0], jitter=None) #0.01)\n",
    "    ]\n",
    "    \n",
    "    classes = range(t_max+1)\n",
    "    model_data = ImageData(EXP_PATH, datasets, bs, num_workers, classes)\n",
    "    return model_data\n",
    "\n",
    "\n",
    "num_evt_types = 1\n",
    "def custom_loss(preds, target):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    evt_times = target[:,0]\n",
    "    evt_types = target[:,1]\n",
    "    l1_loss, pairwise_loss = deephit_loss(preds, evt_times, evt_types, t_max+1, num_evt_types)\n",
    "    b1 = 0.50\n",
    "    b2 = 0.50\n",
    "    return b1 * pairwise_loss + b2 * l1_loss \n",
    "\n",
    "\n",
    "\n",
    "class ConcordanceIndex(Callback):\n",
    "    def __init__(self, learner):\n",
    "        self.learner = learner\n",
    "        self.best_c_index = -np.Inf\n",
    "        self.reset()\n",
    "\n",
    "    def on_epoch_begin(self, metrics):\n",
    "        self.reset()\n",
    "\n",
    "    def on_epoch_end(self, metrics):\n",
    "        ci = concordance_index(\n",
    "            np.array(self.evt_times), \n",
    "            np.array(self.preds), \n",
    "            np.array(self.evt_types)\n",
    "        )\n",
    "        print('ci: ', ci, len(self.preds), len(self.evt_times))\n",
    "        if ci>self.best_c_index:\n",
    "            self.best_c_index = ci\n",
    "            self.learner.save('best_ci_liver_age_1')\n",
    "            print('c index improved!')\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.preds = []\n",
    "        self.evt_times = []\n",
    "        self.evt_types = []\n",
    "        self.mcount = 0\n",
    "       \n",
    "    def concordance_metric(self, preds, target):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        self.evt_times += list(target[:,0])\n",
    "        self.evt_types += list(target[:,1])\n",
    "        self.preds += list(np.argmax(preds, axis=1))\n",
    "        self.mcount += 1\n",
    "        return 0.0 #self.mcount    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model = resnet34\n",
    "bs = 64\n",
    "sz = 256\n",
    "tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_top_down) \n",
    "md = get_data(sz, bs, tfms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitCols(nn.Module):\n",
    "    def __init__(self, img_model, cols_model, sz, n_chan, n_cols):\n",
    "        super().__init__()\n",
    "        self.sz = sz\n",
    "        self.n_cols = n_cols\n",
    "        self.n_chan = n_chan\n",
    "        self.img_model = img_model\n",
    "        self.cols_model = cols_model\n",
    "        self.final = nn.Sequential(\n",
    "            #nn.Linear(in_features=2*len(md.classes), out_features=len(md.classes)),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x_cols = x[:,0:self.n_cols]\n",
    "        #x_img = x[:,self.n_cols:].reshape((bs,self.n_chan,sz,sz))\n",
    "        #img_result = self.img_model(x_img)\n",
    "        cols_result = cols_model(x_cols)\n",
    "        #x_combo = torch.cat((cols_result, img_result), dim=1)\n",
    "        x_combo = cols_result # + img_result\n",
    "        #import pdb; pdb.set_trace()\n",
    "        return self.final(x_combo)\n",
    "\n",
    "\n",
    "feat = 1024\n",
    "\n",
    "layers = [AdaptiveConcatPool2d(), Flatten()]\n",
    "layers += [\n",
    "    BatchNorm1d(feat),\n",
    "    Dropout(p=0.5), \n",
    "    Linear(in_features=feat, out_features=256), \n",
    "    ReLU(), \n",
    "    BatchNorm1d(256),\n",
    "    Dropout(p=0.5), \n",
    "    Linear(in_features=256, out_features=len(md.classes))\n",
    "]\n",
    "custom_head = Sequential(*layers)\n",
    "\n",
    "hidden = 25\n",
    "cols_model = nn.Sequential(\n",
    "    Linear(in_features=n_cols, out_features=hidden), \n",
    "    ReLU(), \n",
    "    BatchNorm1d(hidden),\n",
    "    Dropout(p=0.5), \n",
    "    Linear(in_features=hidden, out_features=len(md.classes))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SplitCols(\n",
      "  (cols_model): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=25, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=25, out_features=3676, bias=True)\n",
      "  )\n",
      "  (final): Sequential(\n",
      "    (0): Softmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "complete_age_model = to_gpu(SplitCols(None,cols_model,sz,2,n_cols))\n",
    "print(complete_age_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "116\n",
      "11904\n",
      "7377\n"
     ]
    }
   ],
   "source": [
    "print(md.trn_dl.batch_size)\n",
    "print(len(md.trn_dl))\n",
    "print(64*186)\n",
    "print(md.trn_dl.dataset.n) #appears that we have 186 batches of 64 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fredmonroe/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Set the logger\n",
    "from aixtras.logger import Logger\n",
    "\n",
    "## inside /bio/liver/notebooks run this:\n",
    "# $tensorboard --logdir='./logs' --port=6006\n",
    "\n",
    "## then open up localhost:6006/ and view the stats as they come in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [25/580], Loss: 101.0651, Acc: 0.00\n",
      "Step [50/580], Loss: 71.2140, Acc: 0.00\n",
      "Step [75/580], Loss: 100.1514, Acc: 0.00\n",
      "Step [100/580], Loss: 104.8883, Acc: 0.00\n",
      "Step [125/580], Loss: 36.2603, Acc: 0.00\n",
      "Step [150/580], Loss: 93.2080, Acc: 0.00\n",
      "Step [175/580], Loss: 67.4753, Acc: 0.00\n",
      "Step [200/580], Loss: 28.9697, Acc: 0.00\n",
      "Step [225/580], Loss: 78.6993, Acc: 0.00\n",
      "Step [250/580], Loss: 78.7906, Acc: 0.02\n",
      "Step [275/580], Loss: 97.2018, Acc: 0.02\n",
      "Step [300/580], Loss: 79.5965, Acc: 0.02\n",
      "Step [325/580], Loss: 75.0766, Acc: 0.03\n",
      "Step [350/580], Loss: 118.8995, Acc: 0.02\n",
      "Step [375/580], Loss: 87.7741, Acc: 0.02\n",
      "Step [400/580], Loss: 98.8862, Acc: 0.03\n",
      "Step [425/580], Loss: 27.0166, Acc: 0.02\n",
      "Step [450/580], Loss: 89.8451, Acc: 0.03\n",
      "Step [475/580], Loss: 100.9382, Acc: 0.02\n",
      "Step [500/580], Loss: 116.2073, Acc: 0.05\n",
      "Step [525/580], Loss: 69.7413, Acc: 0.05\n",
      "Step [550/580], Loss: 112.3434, Acc: 0.05\n",
      "Step [575/580], Loss: 31.7113, Acc: 0.03\n"
     ]
    }
   ],
   "source": [
    "## manual train loop allows us to use TensorBoard and to inspect gradients, etc\n",
    "data_loader = md.trn_dl\n",
    "criterion = custom_loss\n",
    "net = complete_age_model\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "logger = Logger('./logs/age_fit_logger')\n",
    "\n",
    "\n",
    "data_iter = iter(data_loader)\n",
    "iter_per_epoch = len(data_loader)\n",
    "total_epochs = 5\n",
    "total_steps = total_epochs*iter_per_epoch\n",
    "\n",
    "# Start training\n",
    "for step in range(total_steps):\n",
    "    \n",
    "    # Reset the data_iter\n",
    "    if (step+1) % iter_per_epoch == 0:\n",
    "        data_iter = iter(data_loader)\n",
    "\n",
    "    x,y = next(data_iter)\n",
    "    \n",
    "    # Forward, backward and optimize\n",
    "    optimizer.zero_grad()  # zero the gradient buffer\n",
    "    outputs = net(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute accuracy; this probably does not make sense, but I had it from template before...\n",
    "    _, argmax = torch.max(outputs, 1)\n",
    "    accuracy = (y[:,0] == argmax.squeeze()).float().mean()\n",
    "\n",
    "    if (step+1) % 25 == 0:\n",
    "        print ('Step [%d/%d], Loss: %.4f, Acc: %.2f' \n",
    "               %(step+1, total_steps, loss.item(), accuracy.item()))\n",
    "\n",
    "        #============ TensorBoard logging ============#\n",
    "        # (1) Log the scalar values\n",
    "        info = {\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': accuracy.item()\n",
    "        }\n",
    "\n",
    "        for tag, value in info.items():\n",
    "            logger.scalar_summary(tag, value, step+1)\n",
    "\n",
    "        # (2) Log values and gradients of the parameters (histogram)\n",
    "        for tag, value in net.named_parameters():\n",
    "            tag = tag.replace('.', '/')\n",
    "            logger.histo_summary(tag, to_np(value), step+1)\n",
    "            logger.histo_summary(tag+'/grad', to_np(value.grad), step+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.pretrained(f_model, md, custom_head=custom_head)\n",
    "cindex = ConcordanceIndex(learn)\n",
    "callbacks = [cindex]\n",
    "learn.crit = custom_loss\n",
    "learn.metrics = [cindex.concordance_metric]\n",
    "\n",
    "learn.unfreeze()\n",
    "learn.models.model = to_gpu(SplitCols(learn.models.model, cols_model, sz, 3, n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
